---
title: "Exercise #3"
subtitle: "Fortgeschrittene Statistische Software für NF"
author: "Maximilian Neßlauer (Matrikelnummer on Moodle due to Security/Privacy reasons)"
date: "`r Sys.Date()`"
output: distill::distill_article
---

## General Remarks

-   You can submit your solutions in teams of up to 3 students.
-   Include all your team-member's names and student numbers
    (Matrikelnummern) in the `authors` field.
-   Please use the exercise template document to work on and submit your
    results.
-   Use a level 2 heading for each new exercise and answer each subtask
    next to its bullet points or use a new level 3 heading if you want.
-   Always render the R code for your solutions (`echo=TRUE`) and make
    sure to include the resulting data in your rendered document.
    -   Make sure to not print more than 10 rows of data (unless
        specifically instructed to).
-   Always submit both the rendered document(s) as well as your source
    Rmarkdown or Quarto document. Submit the files separately on moodle,
    **not** as a zip archive.
-   Submission format is HTML. Other formats will lead to a deduction of
    points.

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  max.print = 10,
  echo = TRUE
)
if (!require("tidyverse")) install.packages("tidyverse")
library(tidyverse)
if (!require("easystats")) install.packages("easystats")
library(easystats)
```

## Exercise 1: Initializing git (4 Points)

For this whole exercise sheet we will be tracking all our changes to it
in git.

a)  Start by initializing a new R project with git support, called
    `2025-exeRcise-sheet-3`. If you forgot how to do this, you can
    follow this
    [guide](https://malikaihle.github.io/Introduction-RStudio-Git-GitHub/rstudio_project.html).
b)  Commit the files generated by Rstudio.
c)  For all of the following tasks in this exercise sheet we ask you to
    always commit your changes after finishing each subtask e.g. create
    a commit after task *1d*, *1e* etc.

> Note: This applies only to answers that have text or code as their
> answer. If you complete tasks in a different order or forget to commit
> one, this is no problem. If you change your answers you can just
> create multiple commits to track the changes.

d)  Name 2 strengths and 2 weaknesses of git. (Don't forget to create a
    commit after this answer, see *1c*)

**Strengths of git:**

1. **Version Control and History**: Git tracks all changes made to files over time and allows you to review history, revert to previous versions, and understand how code evolved.
2. **Collaboration**: Multiple developers can work on the same project simultaneously through branching and merging and git therefore contributes to making team collaboration efficient and organized.

**Weaknesses of git:**

1. **Learning Curve**: Git has many commands and concepts such as branches, merges, rebases, etc. that can be overwhelming for beginners.
2. **Binary Files**: Git is not well-suited for large binary files (like images, videos, or compiled files).

e)  Knit this exercise sheet. Some new files will automatically be
    generated when knitting the sheet e.g. the HTML page. Ignore these
    files, as we only want to track the source files themselves. You
    can, but don't need to create a `.gitignore` file. Just do not
    commit these files manually.

## Exercise 2: Putting your Repository on GitHub (3 Points)

For this task you will upload your solution to GitHub.

a)  Create a new repository on GitHub in your account named
    `exeRcise-sheet-3`. Make sure you create a **public repository** so
    we are able to see it for grading. Add the link to the repository
    below:

    [Maximilian Neßlauer exeRcise-sheet-3 public GitHub Repository](https://github.com/Maximilian-Nesslauer/exeRcise-sheet-3)

b)  Push your code to this new repository by copying and executing the
    snippet on github listed under
    `…or push an existing repository from the command line`.
c)  Regularly push your latest changes to GitHub again and especially do
    so when you are finished with this sheet.

## Exercise 3: Pixar Films (4 Points)

Download the `pixar_films` and `public_response` datasets from the
GitHub repository and track them in git.

Link:
<https://github.com/rfordatascience/tidytuesday/tree/main/data/2025/2025-03-11>

For small datasets like these adding them to git is not a problem.

a)  Load the `pixar_films` dataset into R. Clean the dataset by removing
    films without a title. Inspect the variable `film_rating`. What are
    the possible values and what do they mean? Create a factor variable
    for the film rating. Why is this appropriate?

```{r exercise-3a}
# Load pixar_films dataset
pixar_films <- read_csv("data/pixar_films.csv")

# Clean dataset by removing films without a title
pixar_films_clean <- pixar_films %>%
  filter(!is.na(film))

# Inspect film_rating variable
unique(pixar_films_clean$film_rating)

# Count of each rating
table(pixar_films_clean$film_rating)

# Create factor variable for film_rating
pixar_films_clean <- pixar_films_clean %>%
  mutate(film_rating_factor = factor(film_rating, 
                                      levels = c("G", "PG", "Not Rated", "N/A"),
                                      ordered = TRUE))

# Display structure to show factor variable
summary(pixar_films_clean$film_rating_factor)
```


**Possible values and their meanings:**

- **G**: General Audiences. All ages admitted
- **PG**: Parental Guidance Suggested. Some material may not be suitable for children
- **Not Rated**: Film has not been rated by the MPAA
- **N/A**: Rating information not available

**Why a factor variable is appropriate:**

Creating a factor variable for film ratings is appropriate because:

1. Film ratings are categorical data with a limited set of discrete values
2. The ratings have a natural ordering (G < PG in terms of content restrictions)
3. Using a factor prevents invalid rating values and therefore ensures data integrity
4. It makes analysis and visualization more easy as R will treat them as distinct categories rather than character strings

> Note: I inclued "Not Rated" in the factor variable because it exists in the CSV. It was filtered out by chance because the movie also has no title.

b)  Inspect the film titles manually. Which films form a film series? A
    film series can be identified by a common word in the titles of the
    films, often in conjunction with a number in the title,
    e.g. "Despicable Me" and "Despicable Me 2". Create a dataframe which
    displays a list of the different series with the titles of the films
    and how many films belong to the series. Output the dataframe.

**Identify film series manually:**

- Toy Story series: Toy Story, Toy Story 2, Toy Story 3, Toy Story 4
- Cars series: Cars, Cars 2, Cars 3
- Finding series: Finding Nemo, Finding Dory
- Incredibles series: The Incredibles, Incredibles 2
- Monsters series: Monsters, Inc., Monsters University

```{r exercise-3b}


# Create a dataframe for film series
film_series <- tibble(
  series_name = c(
    rep("Toy Story", 4),
    rep("Cars", 3),
    rep("Finding", 2),
    rep("Incredibles", 2),
    rep("Monsters", 2)
  ),
  film_title = c(
    "Toy Story", "Toy Story 2", "Toy Story 3", "Toy Story 4",
    "Cars", "Cars 2", "Cars 3",
    "Finding Nemo", "Finding Dory",
    "The Incredibles", "Incredibles 2",
    "Monsters, Inc.", "Monsters University"
  )
)

# Create summary dataframe showing series, film count, and film titles
series_summary <- film_series %>%
  group_by(series_name) %>%
  summarise(
    film_count = n(),
    films = paste(film_title, collapse = ", ")
  ) %>%
  arrange(desc(film_count))

# Output the dataframe
series_summary
```

c)  Load the `public_response` dataframe into R. Convert the
    `cinema_score` variable into a factor while ensuring the factor
    levels are defined in ascending order, from the lowest to the
    highest score. Combine `public_response` with the `pixar_films`
    dataset using an appropriate merge variable.

```{r exercise-3c}
# Load public_response dataset
public_response <- read_csv("data/public_response.csv")

# Inspect unique values of cinema_score
unique(public_response$cinema_score)

# Convert cinema_score to ordered factor (ascending order)
public_response <- public_response %>%
  mutate(cinema_score_factor = factor(cinema_score,
                                      levels = c("A-", "A", "A+", "NA"),
                                      ordered = TRUE))

# Check the factor levels
levels(public_response$cinema_score_factor)

# Combine public_response with pixar_films_clean using film as merge variable
pixar_combined <- pixar_films_clean %>%
  left_join(public_response, by = "film")

# Display structure and first few rows to verify merge
head(pixar_combined)
```

d)  Choose one of the variables representing the public response and
    create a bar plot for the films belonging to a series. Here are the
    details of the plot:

    -   The film series are represented on the x-axis.
    -   Your chosen public response variable is displayed on the y-axis.
    -   Each film in the series is represented as a separate bar. Bars
        are grouped by film under their respective series on the x-axis.
        Order the bars within a series according to the release date of
        the films.
    -   A title and axis labels for context.

    What do you notice when comparing the scores of the films in a
    series? Do you see any patterns?

```{r exercise-3d, fig.cap="Rotten Tomatoes scores for Pixar film series, ordered by release date within each series"}

# Filter for films that belong to film_series and have rotten_tomatoes scores
series_films <- pixar_combined %>%
  filter(film %in% film_series$film_title) %>%
  left_join(film_series, by = c("film" = "film_title")) %>%
  filter(!is.na(rotten_tomatoes)) %>%
  arrange(series_name, release_date)

# Create ordered factor for films to maintain release date order within series
series_films <- series_films %>%
  mutate(film_ordered = factor(film, levels = unique(film)))

# Create bar plot
ggplot(series_films, aes(x = series_name, y = rotten_tomatoes, fill = film_ordered)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.9), width = 0.8) +
  labs(title = "Rotten Tomatoes Scores for Pixar Film Series",
       x = "Film Series",
       y = "Rotten Tomatoes Score (%)",
       fill = "Film") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 30, hjust = 1),
        legend.position = "bottom") +
  scale_y_continuous(limits = c(0, 100)) +
  guides(fill = guide_legend(nrow = 3))
```

**Patterns observed in film series scores:**

1. **Toy Story Series**: Shows exceptional consistency with all four films achieving perfect or near-perfect scores close to 100%. This shows no decline in critical reception.

2. **Cars Series**: Shows the most dramatic variation of any Pixar series as Cars starts at ~74%, falls to just ~40% with Cars 2 (the lowest-rated film of the pixar films), and then recovers to ~69% with Cars 3.
   
3. **Finding Series**: Both films achieve high scores, with Finding Dory just 5 points below finding Nemo.

4. **Incredibles Series**: Both fimils maintain a very high reception despite the 14 year gap between films.

5. **Monsters Series**: A more noticeable decline from Monsters to the prequel is visible, representing a 16-point drop.

**General patterns:**

- Three series (Toy Story, Finding, Incredibles) maintain high scores above 90%
- Most sequels/prequels show some decline from the original, with the notable exception of Toy Story 2
- Despite declines, all films in these series except Cars 2 score very high, demonstrating Pixar's generally strong quality

## Exercise 4: Open Analysis (4 points)

This exercise is a bit more open-ended. You can choose any dataset from
[Our World in Data](https://ourworldindata.org/) and analyze it, while
determining the research question yourself.

a)  Go to <https://github.com/owid/owid-datasets/tree/master/datasets>
    and choose a dataset that interests you. You can have a look at
    <https://ourworldindata.org/> to gather some inspiration.
b)  Download the dataset and track it in git.
c)  Put the name / title of the dataset and a link to it below.

-   Dataset Name: Population covered by the internet - Internet World Stats (2019)
-   Link: [Dataset "Population covered by the internet - Internet World Stats (2019)" on GitHub](https://github.com/owid/owid-datasets/blob/6155d4ca1ea14ef30e753010a25521eeb416e8a2/datasets/Population%20covered%20by%20the%20internet%20-%20Internet%20World%20Stats%20(2019)/Population%20covered%20by%20the%20internet%20-%20Internet%20World%20Stats%20(2019).csv)

d)  Come up with a (research) question you want to answer with the data
    and briefly explain why you believe this is an interesting question
    within one sentence. It should be a question that can be answered
    with the dataset and using R.

    "How does internet coverage vary across different regions of Africa, and which countries are leading or lagging in digital connectivity?"

    I think that this is an important question because internet access is increasingly essential for economic participation, education and social development.
    
e)  Use R to answer your chosen question. Please limit your analysis to
    the functions and techniques we have covered so far in the course.
    You are **not expected** to use advanced statistical models or
    external packages which haven't been introduced.

```{r exercise-4e}
# Load the internet coverage dataset
internet_coverage <- read_csv("data/Population covered by the internet - Internet World Stats (2019).csv")

# Rename columns for easier handling
internet_coverage <- internet_coverage %>%
  rename(
    country = Entity,
    year = Year,
    coverage_percent = `Share of population covered by the internet (in %)`
  )

# Create coverage level categories
internet_coverage <- internet_coverage %>%
  mutate(
    coverage_level = case_when(
      coverage_percent < 10 ~ "Very Low (<10%)",
      coverage_percent >= 10 & coverage_percent < 25 ~ "Low (10-25%)",
      coverage_percent >= 25 & coverage_percent < 50 ~ "Medium (25-50%)",
      coverage_percent >= 50 & coverage_percent < 75 ~ "High (50-75%)",
      coverage_percent >= 75 ~ "Very High (≥75%)"
    ),
    coverage_level = factor(coverage_level, 
                           levels = c("Very Low (<10%)", "Low (10-25%)", 
                                    "Medium (25-50%)", "High (50-75%)", 
                                    "Very High (≥75%)"),
                           ordered = TRUE)
  )

# Summary statistics by coverage level
coverage_summary <- internet_coverage %>%
  group_by(coverage_level) %>%
  summarise(
    count = n(),
    mean_coverage = mean(coverage_percent),
    min_coverage = min(coverage_percent),
    max_coverage = max(coverage_percent)
  )

print(coverage_summary)

# Identify top 10 and bottom 10 countries
top_countries <- internet_coverage %>%
  arrange(desc(coverage_percent)) %>%
  head(10)

bottom_countries <- internet_coverage %>%
  arrange(coverage_percent) %>%
  head(10)

cat("\nTop 10 African countries by internet coverage:\n")
print(top_countries %>% select(country, coverage_percent))

cat("\nBottom 10 African countries by internet coverage:\n")
print(bottom_countries %>% select(country, coverage_percent))

# Calculate regional statistics (grouping by sub-regions based on geographic patterns)
# Note: This is a simplified regional grouping based on observable patterns
internet_coverage <- internet_coverage %>%
  mutate(
    region = case_when(
      country %in% c("Algeria", "Egypt", "Libya", "Morocco", "Tunisia", "Sudan") ~ "North Africa",
      country %in% c("Kenya", "Ethiopia", "Tanzania", "Uganda", "Rwanda", "Burundi", 
                    "Somalia", "Djibouti", "Eritrea") ~ "East Africa",
      country %in% c("Nigeria", "Ghana", "Senegal", "Mali", "Burkina Faso", "Niger",
                    "Benin", "Togo", "Cote d'Ivoire", "Guinea", "Guinea-Bissau",
                    "Liberia", "Sierra Leone", "Gambia", "Mauritania", "Cape Verde") ~ "West Africa",
      country %in% c("Cameroon", "Gabon", "Congo", "Democratic Republic of Congo",
                    "Central African Republic", "Chad", "Equatorial Guinea",
                    "Sao Tome and Principe") ~ "Central Africa",
      country %in% c("South Africa", "Botswana", "Namibia", "Zimbabwe", "Zambia",
                    "Angola", "Mozambique", "Malawi", "Lesotho", "Swaziland",
                    "Madagascar", "Mauritius", "Comoros", "Seychelles", "Mayotte",
                    "Reunion", "Saint Helena") ~ "Southern Africa",
      TRUE ~ "Other"
    )
  )

# Regional summary
regional_summary <- internet_coverage %>%
  filter(region != "Other") %>%
  group_by(region) %>%
  summarise(
    countries = n(),
    mean_coverage = mean(coverage_percent),
    median_coverage = median(coverage_percent),
    sd_coverage = sd(coverage_percent)
  ) %>%
  arrange(desc(mean_coverage))

cat("\nInternet coverage by African region:\n")
print(regional_summary)
```
f)  Create a meaningful plot / figure with the dataset. Make sure to
    provide a figure caption (via the chunk options / Rmarkdown) and
    correctly label the figure.

## Final Note

Make sure to push all your commits and changes to GitHub before
submitting the exercise sheet.
